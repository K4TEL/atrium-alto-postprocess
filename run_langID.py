#!/usr/bin/env python3
"""
run_langID.py

Purpose:
This is the main script for performing line-by-line text extraction,
language identification (LID), and quality assessment.

It reads an input CSV (typically generated by 'alto_stats_create.py'),
which must contain 'file', 'page', and 'path' columns.

For each row (representing one page):
1.  It extracts all text lines from the ALTO XML file specified in the 'path'
    (using the `get_text_from_alto` utility).
2.  It runs the batch classification function (`classify_lines_batch`)
    on all lines from that page.
3.  This classification uses 'fastText' for language ID and 'distilgpt2'
    for perplexity (quality) scoring.
4.  It aggregates the results for the page (e.g., counting how many
    'Clear', 'Noisy', 'Trash' lines there are).

Outputs:
This script is configured to produce several outputs:

1.  OUTPUT_STATS_FILE (e.g., "line_counts_alto_statistics.csv"):
    The input CSV augmented with new columns for line counts
    (e.g., 'clear_lines', 'noisy_lines', 'trash_lines', 'languages').

2.  OUTPUT_LINES_FILE (e.g., "pages_classified_alto_statistics.csv"):
    A new, very large CSV containing one row for *every single text line*
    processed, detailing its text, language, perplexity, and category.

3.  OUTPUT_TEXT_DIR (e.g., "../PAGE_TXT"):
    A folder containing the raw extracted text (.txt) for each page,
    which acts as a cache.

4.  OUTPUT_STAT_DIR (e.g., "../PAGE_STAT"):
    A folder where the two main CSV outputs (1 and 2) are split into
    smaller, per-document files (e.g., "line_counts_doc123.csv").
"""

# Import all functions from our utility file
from text_util import *
import time  # For timing the process


# --- Output Categories (Documentation) ---
# Each line of text is assigned one of the following categories:
# - Clear:    High-confidence, low-perplexity, common language.
# - Rough:    Medium-confidence, but still likely a common language.
# - Noisy:    Low-confidence, high-perplexity, or other indicators of OCR issues.
# - Trash:    Very high perplexity, or non-prose (e.g., all-caps headers).
# - Short:    Too few words (< 3) to make a confident classification.
# - Non-text: Failed heuristic checks (e.g., mostly digits/symbols).
# - Empty:    Line contains only whitespace.
# - N/A:      Used internally for error cases or placeholders.
# -------------------------------------------


def main():
    """
    (NEW, BATCH-OPTIMIZED VERSION)
    Main processing logic. Reads the input CSV in chunks, processes each page,
    classifies all lines on that page *in a single batch*, and writes
    to three separate outputs.
    """
    # --- 1. Configuration ---
    # (These are hardcoded, but could be moved to command-line arguments)
    INPUT_FILE = "test_alto_stats.csv"  # Input CSV from alto_stats_create.py
    OUTPUT_TEXT_DIR = "../PAGE_TXT"  # Output (3) - folder for raw .txt files
    OUTPUT_STAT_DIR = "../PAGE_STAT"  # Output (4) - folder for per-document CSVs
    CHUNK_SIZE = 500  # How many rows (pages) to read into memory at a time
    LOG_STEP = 10  # How often to print detailed progress (every 10 pages)

    MODEL_PATH = "lid.176.bin"  # Path to downloaded fastText model

    # Output (1) - The summary stats file
    OUTPUT_STATS_FILE = "line_counts_" + Path(INPUT_FILE).name
    # Output (2) - The detailed per-line file
    OUTPUT_LINES_FILE = "pages_classified_" + Path(INPUT_FILE).name

    # (COMMON_LANGS is defined in text_util.py)

    # --- 2. Load Spellers ---
    # Pre-load spellers for common languages so we don't reload them
    # for every single line.
    SPELLERS = {}
    for common_lang in COMMON_LANGS:
        speller_type, speller_lang = speller_per_language[common_lang]
        if speller_type == 1:
            SPELLERS[common_lang] = Speller(speller_lang, only_replacements=True)
        else:
            SPELLERS[common_lang] = SpellChecker(language=speller_lang)
    print(f"Spellcheckers initialized for languages: {', '.join(SPELLERS.keys())}")

    # --- 3. Setup ML Device ---
    # Check if a GPU is available (much faster)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # --- 4. Load Models ---
    print("Loading fastText model (fasttext)...")
    try:
        model_fasttext = fasttext.load_model(MODEL_PATH)
        print("fastText model loaded.")
    except ValueError as e:
        print(f"Error loading model: {e}", file=sys.stderr)
        print(f"Please ensure the model file '{MODEL_PATH}' is in the correct directory.", file=sys.stderr)
        sys.exit(1)

    print("Loading causal LM (distilgpt2) for perplexity...")
    model_id = "distilgpt2"
    model_causal = AutoModelForCausalLM.from_pretrained(model_id).to(device)
    tokenizer_causal = AutoTokenizer.from_pretrained(model_id)
    # Set padding token for batch perplexity
    tokenizer_causal.pad_token = tokenizer_causal.eos_token
    print("Causal LM loaded.")

    # --- 5. Setup Output Directories ---
    print(f"Starting processing of '{INPUT_FILE}'")
    Path(OUTPUT_TEXT_DIR).mkdir(parents=True, exist_ok=True)
    print(f"Raw text outputs will be saved to '{OUTPUT_TEXT_DIR}/'")
    Path(OUTPUT_STAT_DIR).mkdir(parents=True, exist_ok=True)
    print(f"Document level results will be saved to '{OUTPUT_STAT_DIR}/'")

    try:
        # --- 6. Start Processing ---
        print(f"Analyzing '{INPUT_FILE}'...")
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            total_rows = sum(1 for _ in f) - 1  # Count rows for progress tracking
        if total_rows <= 0:
            print("Input file is empty. Exiting.")
            return
        print(f"Found {total_rows:,} rows (pages) to process.")

        # Open the detailed lines CSV for writing
        with open(OUTPUT_LINES_FILE, 'w', encoding='utf-8', newline='') as f_lines_csv:
            lines_writer = csv.writer(f_lines_csv)
            # Write header for the detailed lines file
            lines_writer.writerow(["file", "page", "line", "line_text",
                                   "lang_code", "lang_corrected",
                                   "lang_score", "lang_score_corrected",
                                   "perplexity", "perplexity_corrected",
                                   "category", "corrected_text"])

            rows_processed, txt_lines_processed = 0, 0
            # This will store the DataFrames for each chunk
            all_stats_chunks = []

            # Process the input stats file in chunks
            print(f"Reading '{INPUT_FILE}' in chunks of {CHUNK_SIZE}...")
            reader = pd.read_csv(INPUT_FILE, chunksize=CHUNK_SIZE)
            time_start = time.time()

            # --- 7. Chunk Loop ---
            for chunk_df in reader:
                if "path" not in chunk_df.columns:
                    raise ValueError("Input CSV must have 'path' column.")

                new_stats_rows = []  # To build the new stats for this chunk

                # --- 8. Row Loop (Page-by-Page) ---
                for idx, row in chunk_df.iterrows():
                    file_id = str(row['file'])
                    page_id = str(row['page'])
                    xml_path = row['path']

                    # Define the path for the cached .txt file
                    xml_file_directory = Path(xml_path).parent
                    separate_dir = str(xml_file_directory).endswith(file_id)
                    if not separate_dir:
                        page_text_file = Path(OUTPUT_TEXT_DIR) / f"{file_id}-{page_id}.txt"
                    else:
                        page_text_file = Path(OUTPUT_TEXT_DIR) / file_id / f"{file_id}-{page_id}.txt"
                        page_text_file.parent.mkdir(parents=True, exist_ok=True)

                    # --- A. Extract Text from ALTO ---
                    # (This will use the .txt cache if it exists)
                    page_lines = get_text_from_alto(xml_path, page_text_file)
                    total_text = " ".join(page_lines)  # For page-level stats

                    # --- B. Save raw text file (to complete the cache) ---
                    if not page_text_file.exists():
                        try:
                            with open(page_text_file, 'w', encoding='utf-8') as f_txt:
                                f_txt.write("\n".join(page_lines))
                        except Exception as e:
                            print(f"\n[Warning] Failed to write text file {page_text_file}: {e}", file=sys.stderr)

                    # --- C. Pre-calculate Page-Level stats ---
                    # These are used as context for 'Short' lines
                    try:
                        total_text_perplexity = calculate_perplexity(total_text, model_causal, tokenizer_causal, device)
                        labels, scores = model_fasttext.predict(total_text, k=1)
                        total_lang_code = labels[0].replace("__label__", "")
                        total_score1 = float(scores[0])
                    except Exception as e:
                        total_lang_code = ""
                        total_score1 = 0.0
                        total_text_perplexity = 0.0
                        print(f"\n[Error] Page-level prediction failed for {file_id}, page {page_id}: {e}",
                              file=sys.stderr)

                    # --- D. Process all lines on the page (BATCH CALL) ---
                    line_counts = {"Clear": 0, "Noisy": 0, "Trash": 0, "Non-text": 0, "Empty": 0, "Rough": 0,
                                   "Short": 0}
                    language_counts = {lang: 0 for lang in COMMON_LANGS}
                    language_counts['other'] = 0

                    if len(page_lines) == 0:
                        print(f"\t[Warning] No text lines extracted from file '{file_id}', page '{page_id}'")
                        # Still need to add a row to the stats output
                        new_row = row.to_dict()
                        new_row['clear_lines'] = 0  # ... all counts 0
                        new_row['noisy_lines'] = 0
                        new_row['trash_lines'] = 0
                        new_row['nontxt_lines'] = 0
                        new_row['empty_lines'] = 0
                        new_row['rough_lines'] = 0
                        new_row['short_lines'] = 0
                        new_row['languages'] = ""
                        new_stats_rows.append(new_row)

                    else:
                        # --- THIS IS THE CORE BATCH CALL ---
                        # Classify all lines on the page at once
                        all_line_results = classify_lines_batch(
                            page_lines,
                            model_ft=model_fasttext,
                            model_ppl=model_causal,
                            tokenizer_ppl=tokenizer_causal,
                            corrected_page_lang=total_lang_code,
                            corrected_page_lang_score=total_score1,
                            corrected_page_perplexity=total_text_perplexity,
                            spellers=SPELLERS,
                            device=device
                        )

                        # --- Iterate over RESULTS ---
                        for l_num, result in enumerate(all_line_results, start=1):
                            # Add to counts for the summary file
                            if result['category'] in line_counts:
                                line_counts[result['category']] += 1
                            else:
                                line_counts[result['category']] = 1

                            # Add to language counts
                            lang_saved = False
                            for lang_name in language_counts.keys():
                                if result['lang_code'].startswith(lang_name):
                                    language_counts[lang_name] += 1
                                    lang_saved = True
                                    break
                            if not lang_saved:
                                language_counts['other'] += 1

                            # Write line result to the detailed CSV
                            lines_writer.writerow([
                                file_id, page_id, l_num, result['text'], result['lang_code'], result['lang_corrected'],
                                result['lang_score'], result['lang_score_corrected'],
                                result['perplexity'], result["perplexity_corrected"], result['category'],
                                result['corrected_text']
                            ])

                        # --- E. Prepare new row for the stats file ---
                        new_row = row.to_dict()  # Start with all old data
                        # Add new aggregated data
                        new_row['clear_lines'] = line_counts['Clear']
                        new_row['noisy_lines'] = line_counts['Noisy']
                        new_row['trash_lines'] = line_counts['Trash']
                        new_row['nontxt_lines'] = line_counts['Non-text']
                        new_row['empty_lines'] = line_counts['Empty']
                        new_row['rough_lines'] = line_counts['Rough']
                        new_row['short_lines'] = line_counts['Short']

                        # Get top 2 languages
                        sorted_langs = sorted(language_counts.items(), key=lambda x: x[1], reverse=True)
                        top_langs = [lang for lang, count in sorted_langs if count > 0][:2]
                        new_row['languages'] = "-".join(top_langs) if top_langs else ""

                        new_stats_rows.append(new_row)

                    # print(f"\tProcessed file '{file_id}', page '{page_id}' with '{len(page_lines)}' lines...")

                    # --- F. Log progress ---
                    rows_processed += 1
                    txt_lines_processed += len(page_lines)
                    percentage = (rows_processed / total_rows) * 100
                    time_spent = time.time() - time_start
                    time_minutes = time_spent / 60

                    # Print a compact progress bar
                    sys.stdout.write(
                        f"\r* Progress: [ {rows_processed:,} / {total_rows:,} ] pages ({percentage:.1f}%) processed in [{round(time_minutes, 1)} min]... ")
                    sys.stdout.flush()

                    # Print a detailed log every LOG_STEP rows
                    if rows_processed % LOG_STEP == 0:
                        f_lines_csv.flush()  # Save detailed CSV progress
                        time_per_file = time_spent / rows_processed if rows_processed > 0 else 0
                        time_per_line = time_spent / txt_lines_processed if txt_lines_processed > 0 else 0
                        print(f"\n  -> Time spent so far: {time_spent:.1f} sec ({time_minutes:.1f} min)")
                        print(f"  -> Avg time per PAGE: {time_per_file:.2f} sec")
                        print(f"  -> Avg time per LINE: {time_per_line:.2f} sec")
                        print(
                            f"\tResults for file '{file_id}', page '{page_id}' ({len(page_lines)} lines):\n\t{line_counts}\n\tLanguages: {new_row.get('languages', 'N/A')}\n")

                # --- 9. End of Chunk ---
                # Add the completed stats rows for this chunk to the main list
                all_stats_chunks.append(pd.DataFrame(new_stats_rows))
                f_lines_csv.flush()  # Save detailed CSV progress

        # --- 10. Post-Processing and Saving ---
        print("\n\nProcessing complete.")
        time_total = time.time() - time_start
        # ... (print final time stats) ...
        time_minutes_total = time_total / 60
        time_hours_total = time_total / 3600
        print(f"Total time: {time_total:.1f} sec ({time_minutes_total:.1f} min, {time_hours_total:.2f} hr)")

        # --- A. Save the main stats file ---
        if all_stats_chunks:
            print("Consolidating stats file...")
            # Combine all the chunk DataFrames into one big DataFrame
            final_stats_df = pd.concat(all_stats_chunks, ignore_index=True)

            print(f"Sorting '{OUTPUT_STATS_FILE}' by file, page...")
            final_stats_df.sort_values(by=["file", "page"], inplace=True)
            final_stats_df.to_csv(OUTPUT_STATS_FILE, index=False, header=True)
            print(f"Updated stats with line counts saved to: {OUTPUT_STATS_FILE}")

            # --- B. Sort the detailed lines file ---
            # This requires re-reading the *entire* detailed CSV, sorting it,
            # and re-saving it. This can be slow and memory-intensive.
            print(f"Sorting '{OUTPUT_LINES_FILE}' by file, page, and line. This may take a moment...")
            try:
                df_lines = pd.read_csv(OUTPUT_LINES_FILE)
                df_lines.sort_values(by=["file", "page", "line"], inplace=True)
                df_lines.to_csv(OUTPUT_LINES_FILE, index=False, header=True)
                print("Detailed lines file sorting complete.")
                print(f"Detailed line-by-line analysis saved to: {OUTPUT_LINES_FILE}")

                # --- C. Split results into per-document tables ---
                print(f"Splitting total CSVs into per-document tables in '{OUTPUT_STAT_DIR}'...")
                all_file_ids = final_stats_df['file'].unique()
                split_count = 0
                for file_id in all_file_ids:
                    file_id_str = str(file_id)
                    doc_stats_path = Path(OUTPUT_STAT_DIR) / f"line_counts_{file_id_str}.csv"
                    doc_lines_path = Path(OUTPUT_STAT_DIR) / f"pages_classified_{file_id_str}.csv"

                    # Filter stats DataFrame for this file_id
                    doc_stats_df = final_stats_df[final_stats_df['file'] == file_id]
                    if not doc_stats_df.empty:
                        doc_stats_df.to_csv(doc_stats_path, index=False, header=True)

                    # Filter detailed lines DataFrame for this file_id
                    doc_lines_df = df_lines[df_lines['file'] == file_id]
                    if not doc_lines_df.empty:
                        doc_lines_df.to_csv(doc_lines_path, index=False, header=True)

                    split_count += 1
                print(f"Successfully split results into {split_count} per-document sets.")

            except Exception as e:
                print(f"\n[Error] Failed to sort {OUTPUT_LINES_FILE} or split per-document files: {e}", file=sys.stderr)

        else:
            print("No data processed, stats file and per-document files not created.")

        print(f"Raw text files saved in: {OUTPUT_TEXT_DIR}/")

    except FileNotFoundError:
        print(f"\n[Error] Input file not found at '{INPUT_FILE}'", file=sys.stderr)
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()