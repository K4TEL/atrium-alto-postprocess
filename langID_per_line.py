#!/usr/bin/env python3
"""
Processes a CSV file of ALTO XML paths to perform line-by-line text extraction,
language identification (LID), and quality assessment.

This script reads an input CSV (generated by 'alto_stats_create.py'), extracts
text from each ALTO file, and analyzes each text line to determine its
language, perplexity (a measure of text "normality"), and a quality category.

It uses:
- 'alto-tools' (external command) for text extraction.
- 'fastText' for language identification.
- 'transformers' (distilgpt2) for perplexity calculation.
- 'autocorrect' and 'spellchecker' for text correction heuristics.

Outputs:
1.  OUTPUT_STATS_FILE: The input CSV augmented with line counts for each
    quality category (e.g., 'clear_lines', 'noisy_lines').
2.  OUTPUT_LINES_FILE: A detailed CSV containing every processed line, its
    text, language, perplexity, and assigned category.
3.  OUTPUT_TEXT_DIR: A folder containing raw extracted text (.txt) for each
    processed page.
"""

import pandas as pd
import fasttext
import sys
import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import subprocess
import os
import csv
from pathlib import Path
from autocorrect import Speller
from spellchecker import SpellChecker

# --- Configuration ---
MODEL_PATH = "lid.176.bin"  # path to downloaded fastText model
INPUT_FILE = "test_alto_stats.csv"  # Input CSV from alto_stats_create.py (must have 'file', 'page', 'path')
OUTPUT_STATS_FILE = "line_counts_" + INPUT_FILE  # Output (1) - stats file with line counts, sorted by file/page
OUTPUT_LINES_FILE = "pages_classified_" + INPUT_FILE  # Output (2) - detailed CSV with all lines, sorted by file/page/category
OUTPUT_TEXT_DIR = "page_texts"  # Output (3) - folder for raw extracted text-per-page
CHUNK_SIZE = 500  # How many rows of the input CSV to read at a time
COMMON_LANGS = ["ces", "deu", "eng"]  # Languages considered "common"

# Perplexity Thresholds for 'distilgpt2'
# Perplexity measures how well a model "predicts" a text.
# Lower perplexity = more "normal" or "expected" text.
# Higher perplexity = more "surprising" or "abnormal" text (e.g., OCR errors, non-text).
PERPLEXITY_THRESHOLD_MAX = 500  # Perplexity >= this -> likely "Trash"
PERPLEXITY_THRESHOLD_MIN = 100  # Perplexity >= this -> likely "Noisy"

# Language Score Thresholds
# fastText language identification returns a score between 0 and 1.
# Higher score = more confidence in the predicted language.
LANG_SCORE_ROUGH = 0.5  # Language score >= this -> "Rough"
LANG_SCORE_CLEAR = 0.9  # Language score >= this -> "Clear"

# Spell-checkers for correction heuristics
SPELLERS = [Speller("en", only_replacements=True), Speller("cs", only_replacements=True), SpellChecker(language='de')]

# --- Output Categories ---
# Each line of text is assigned one of the following categories:
# - Clear:    High-confidence, low-perplexity, common language.
# - Rough:    Medium-confidence, but still likely a common language.
# - Noisy:    Low-confidence, high-perplexity, or other indicators of OCR issues.
# - Trash:    Hard to guess language, very high perplexity, or non-prose (not plain text).
# - Short:    Too few words to make a confident classification.
# - Non-text: Failed heuristic checks (e.g., mostly digits/symbols, very short).
# - Empty:    Line contains only whitespace.
# - N/A:      Used internally for error cases or placeholders.
# -------------------------

# It's recommended to run this on a GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Using device: {device}")

# --- Load Models ---
print("Loading fastText model (fasttext)...")
try:
    model_fasttext = fasttext.load_model(MODEL_PATH)
    print("fastText model loaded.")
except ValueError as e:
    print(f"Error loading model: {e}", file=sys.stderr)
    print(f"Please ensure the model file '{MODEL_PATH}' is in the correct directory.", file=sys.stderr)
    sys.exit(1)

print("Loading causal LM (distilgpt2) for perplexity...")
model_id = "distilgpt2"
model_causal = AutoModelForCausalLM.from_pretrained(model_id).to(device)
tokenizer_causal = AutoTokenizer.from_pretrained(model_id)
print("Causal LM loaded.")


def get_text_from_alto(xml_path: str) -> list[str]:
    """
    Runs the 'alto-tools -t' command on a given ALTO XML file to extract
    all text lines.

    Args:
        xml_path: The file path to the ALTO XML.

    Returns:
        A list of strings, where each string is a stripped text line.
        Returns an empty list if the file is not found or 'alto-tools' fails.
    """
    if not os.path.exists(xml_path):
        print(f"[Warning] ALTO file not found: {xml_path}", file=sys.stderr)
        return []

    cmd = ["alto-tools", "-t", xml_path]
    try:
        # Run alto-tools to extract text
        result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', check=True)
        # Split text into lines, strip whitespace, and filter out empty lines
        lines = [line.strip() for line in result.stdout.splitlines() if line.strip()]
        return lines
    except subprocess.CalledProcessError as e:
        print(f"[Error] alto-tools failed on {xml_path}: {e.stderr}", file=sys.stderr)
        return []
    except FileNotFoundError:
        print("[Error] 'alto-tools' command not found.", file=sys.stderr)
        print("Please ensure 'alto-tools' is installed and in your system's PATH.", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"[Error] Unexpected error processing {xml_path}: {e}", file=sys.stderr)
        return []


def autocorrect_text(input_text: str, lang_code: str) -> tuple[str, list]:
    """
    Attempts to spell-correct a line of text using a language-specific speller.

    Note: The 'pyenchant' or 'autocorrect' libraries have different APIs.
    The German 'spellchecker' logic is different from 'autocorrect' (en, cs).

    Args:
        input_text: The text to correct.
        lang_code: The detected language code (e.g., "eng", "ces", "deu").

    Returns:
        A tuple:
        (corrected_text: str, word_candidates: list)
        where word_candidates is a list of (word_index, [candidates_list])
    """
    speller = None
    if lang_code.startswith("en"):
        speller = SPELLERS[0]
    elif lang_code.startswith("ces"):
        speller = SPELLERS[1]
    elif lang_code.startswith("deu"):
        speller = SPELLERS[2]
    else:
        # No speller for this language
        return input_text, []

    word_candidates = []
    input_words = input_text.split(" ")

    if not lang_code.startswith("deu"):
        # Use 'autocorrect' (Speller) for en, cs
        corrected = speller(input_text)
        for iw, word in enumerate(input_words):
            wc = speller.get_candidates(word)
            if len(wc) > 0:
                word_candidates.append((iw, wc))
    else:
        # Use 'pyspellchecker' (SpellChecker) for deu
        corrected_words = []
        for iw, word in enumerate(input_words):
            # pyspellchecker is case-sensitive, so we check lower
            # but preserve original case if it's correct
            if word in speller:
                corrected_words.append(word)
            else:
                cor = speller.correction(word)
                wc = speller.candidates(word)
                if cor is None:
                    cor = word  # No correction found
                corrected_words.append(cor)
                if wc is not None and len(wc) > 0:
                    word_candidates.append((iw, wc))
        corrected = " ".join(corrected_words)

    return corrected.strip(), word_candidates


def calculate_perplexity(text: str, model=model_causal, tokenizer=tokenizer_causal) -> float:
    """
    Calculates the perplexity of a single line of text using the causal LM.

    Perplexity is calculated as exp(cross-entropy_loss).
    A high value indicates the text is "surprising" or "unlikely"
    (e.g., OCR errors, jumbled text).
    A low value indicates the text is "normal" or "expected" by the model.

    Args:
        text: The input text string.
        model: The pre-loaded AutoModelForCausalLM.
        tokenizer: The pre-loaded AutoTokenizer.

    Returns:
        The perplexity score as a float. Returns 0.0 for empty strings or
        in case of an error.
    """
    if not text:
        return 0.0  # No perplexity for empty strings

    try:
        # Tokenize the text
        encodings = tokenizer(text, return_tensors="pt")
        input_ids = encodings.input_ids.to(device)

        if input_ids.size(1) == 0:
            return 0.0  # Handle cases where text is just whitespace

        # The maximum sequence length the model can handle
        max_length = model.config.max_position_embeddings

        # If text is too long, truncate it
        if input_ids.size(1) > max_length:
            input_ids = input_ids[:, :max_length]

        # For a causal LM, labels are the same as input_ids
        target_ids = input_ids.clone()

        with torch.no_grad():
            outputs = model(input_ids, labels=target_ids)
            # Loss is the average negative log likelihood
            neg_log_likelihood = outputs.loss

        # Perplexity is the exponentiation of the loss
        ppl = torch.exp(neg_log_likelihood)
        return ppl.item()

    except Exception as e:
        print(f"\n[Error] Perplexity calculation failed for text: '{text[:50]}...': {e}", file=sys.stderr)
        return 0.0  # Return 0.0 on error


def classify_line(line: str, model_ft=model_fasttext, model_ppl=model_causal, tokenizer_ppl=tokenizer_causal,
                  corrected_page_text="", corrected_page_lang="", corrected_page_lang_score=0.0,
                  corrected_page_perplexity=0.0) -> dict:
    """
    Detects language, perplexity, and quality category for a single line of text.

    Args:
        line: The raw text line to classify.
        model_ft: The pre-loaded fastText model.
        model_ppl: The pre-loaded causal LM.
        tokenizer_ppl: The pre-loaded tokenizer.
        corrected_page_text (str): The autocorrected text for the *entire page*.
        corrected_page_lang (str): The language code for the *entire page*.
        corrected_page_lang_score (float): The language score for the *entire page*.
        corrected_page_perplexity (float): The perplexity for the *entire page*.
            **Note**: The 'corrected_page_*' args are used as fallback data for lines
            categorized as "Short", which are too short to classify reliably
            on their own.

    Returns:
        A dictionary containing all classification results for the line.
    """
    clean_text = line.strip()

    # --- 1. Handle Empty Line ---
    if not clean_text:
        return {
            "text": line, "lang_code": "N/A", "lang_score": 0.0,
            "perplexity": 0.0, "category": "Empty",
            "corrected_text": "", "lang_corrected": "", "lang_score_corrected": 0.0,
            "perplexity_corrected": 0.0
        }

    # --- 2. Heuristic Pre-check (Quickly filter out non-text) ---
    n_chars = len(clean_text)
    n_words = len(clean_text.split(" "))
    letters = sum(c.isalpha() for c in clean_text)
    digits = sum(c.isdigit() for c in clean_text)
    symbols = sum(not c.isalnum() and not c.isspace() for c in clean_text)
    unique_symbols = set(c for c in clean_text)
    space_chars = sum(c.isspace() for c in clean_text)

    # Avoid division by zero if n_chars is somehow 0
    if n_chars == 0 or space_chars == n_chars:
        return {
            "text": line, "lang_code": "N/A", "lang_score": 0.0,
            "perplexity": 0.0, "category": "Empty",
            "corrected_text": "", "lang_corrected": "", "lang_score_corrected": 0.0,
            "perplexity_corrected": 0.0
        }

    letter_ratio = letters / n_chars
    digit_ratio = digits / n_chars
    symbol_ratio = symbols / n_chars

    # These heuristics identify lines that are very unlikely to be prose.
    if (letter_ratio < 0.3 or  # Less than 30% letters
            digit_ratio > 0.4 or  # More than 40% digits
            symbol_ratio > 0.5 or  # More than 50% symbols
            len(clean_text) < 4 or  # Less than 4 characters
            len(unique_symbols) < 3):  # Less than 3 unique non-space characters
        return {
            "text": line, "lang_code": "N/A", "lang_score": 0.0,
            "perplexity": 0.0, "category": "Non-text",
            "corrected_text": "", "lang_corrected": "", "lang_score_corrected": 0.0,
            "perplexity_corrected": 0.0
        }

    # --- 3. Run Predictions on Original Text ---
    try:
        # Perplexity
        text_perplexity = calculate_perplexity(clean_text, model_ppl, tokenizer_ppl)

        # Language prediction
        labels, scores = model_ft.predict(clean_text, k=1)
        lang_code = labels[0].replace("__label__", "")
        score1 = float(scores[0])

    except Exception as e:
        print(f"\n[Error] Model prediction failed for line: '{line[:50]}...': {e}", file=sys.stderr)
        return {
            "text": line, "lang_code": "N/A", "lang_score": 0.0,
            "perplexity": 0.0, "category": "Trash",  # Fail to "Trash"
            "corrected_text": "", "lang_corrected": "", "lang_score_corrected": 0.0,
            "perplexity_corrected": 0.0
        }

    # --- 4. Handle "Short" lines ---
    # Lines with < 3 words are too short for reliable categorization.
    # We assign them the 'Short' category and backfill the 'corrected'
    # fields with the stats from the *entire page* for context.
    if n_words < 3:
        return {
            "text": line, "lang_code": lang_code, "lang_score": score1,
            "perplexity": text_perplexity, "category": "Short",
            "corrected_text": corrected_page_text,  # Use page-level context
            "lang_corrected": corrected_page_lang,
            "lang_score_corrected": corrected_page_lang_score,
            "perplexity_corrected": corrected_page_perplexity
        }

    # --- 5. Run Predictions on Corrected Text (for comparison) ---
    corrected_text, candidates = autocorrect_text(clean_text, lang_code)

    if corrected_text == clean_text:
        # No corrections were made
        corrected_text = ""
        corrected_text_perplexity = 0.0
        corrected_lang_code = ""
        correcte_score1 = 0.0
    else:
        # Corrections were made, so let's analyze the corrected text
        try:
            corrected_text_perplexity = calculate_perplexity(corrected_text, model_ppl, tokenizer_ppl)
            labels, scores = model_ft.predict(corrected_text, k=1)  # Note: predicting on *corrected* text
            corrected_lang_code = labels[0].replace("__label__", "")
            correcte_score1 = float(scores[0])

        except Exception as e:
            print(f"\n[Error] Model prediction failed for corrected line: '{line[:50]}...': {e}", file=sys.stderr)
            # If correction analysis fails, just return original results and "Trash"
            return {
                "text": line, "lang_code": lang_code, "lang_score": score1,
                "perplexity": text_perplexity, "category": "Trash",
                "corrected_text": "", "lang_corrected": "", "lang_score_corrected": 0.0,
                "perplexity_corrected": 0.0
            }

    # --- 6. Categorize (Clear, Noisy, Trash, Rough) ---
    category = "Clear"  # Start with optimistic assumption

    # Gather features for categorization
    upper_ratio = sum(c.isupper() for c in clean_text) / letters if letters > 0 else 0.0
    is_Latin = lang_code.split("_")[-1] == "Latn"
    is_common = lang_code.split("_")[0] in COMMON_LANGS
    is_cor_Latin = corrected_lang_code.split("_")[-1] == "Latn" if corrected_lang_code else is_Latin
    is_cor_common = corrected_lang_code.split("_")[0] in COMMON_LANGS if corrected_lang_code else is_common

    # Check for TRASH (highest priority)
    if text_perplexity >= PERPLEXITY_THRESHOLD_MAX:
        if corrected_text_perplexity != 0 and corrected_text_perplexity >= PERPLEXITY_THRESHOLD_MAX:
            category = "Trash"  # Extremely high perplexity
    elif upper_ratio > 0.9 and letters > 10:
        category = "Trash"  # All caps (likely headers, not prose)
    elif not is_Latin and not is_cor_Latin:
        category = "Trash"  # Not a Latin script (e.g., Cyrillic, Greek)

    # Check for NOISY
    elif text_perplexity >= PERPLEXITY_THRESHOLD_MIN:
        if corrected_text_perplexity != 0 and  corrected_text_perplexity >= PERPLEXITY_THRESHOLD_MIN:
            category = "Noisy"  # High perplexity, and correction makes it *worse*
    elif upper_ratio > 0.6 and letters > 10:
        category = "Noisy"  # Mostly caps
    elif not is_common and not is_cor_common:
        category = "Noisy"  # Not one of our common languages
    elif score1 < LANG_SCORE_ROUGH and correcte_score1 < LANG_SCORE_ROUGH:
        category = "Noisy"  # Low confidence in *both* original and corrected

    # Check for ROUGH (better than Noisy, not as good as Clear)
    if correcte_score1 > LANG_SCORE_ROUGH and any(corrected_lang_code.startswith(cl) for cl in COMMON_LANGS):
        category = "Rough"  # Correction results in a decent, common lang
    elif score1 > LANG_SCORE_ROUGH and any(lang_code.startswith(cl) for cl in COMMON_LANGS):
        category = "Rough"  # Original is a decent, common lang

    # Check for CLEAR (highest quality)
    if score1 > LANG_SCORE_CLEAR and any(lang_code.startswith(cl) for cl in COMMON_LANGS):
        category = "Clear"  # High-confidence, common language

    return {
        "text": line,
        "corrected_text": corrected_text,
        "lang_code": lang_code,
        "lang_corrected": corrected_lang_code,
        "lang_score": score1,
        "lang_score_corrected": correcte_score1,
        "perplexity": text_perplexity,
        "perplexity_corrected": corrected_text_perplexity,
        "category": category
    }


def main():
    """
    Main processing logic. Reads the input CSV in chunks, processes each page,
    classifies each line, and writes to three separate outputs.
    """
    # --- 1. Setup ---
    print(f"Starting processing of '{INPUT_FILE}'")
    Path(OUTPUT_TEXT_DIR).mkdir(parents=True, exist_ok=True)
    print(f"Raw text outputs will be saved to '{OUTPUT_TEXT_DIR}/'")

    try:
        # 1. Count total rows for progress tracking
        print(f"Analyzing '{INPUT_FILE}'...")
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            total_rows = sum(1 for _ in f) - 1  # Subtract 1 for the header
        if total_rows <= 0:
            print("Input file is empty or has only a header. Exiting.")
            return
        print(f"Found {total_rows:,} rows (pages) to process.")

        # 2. Open the detailed lines CSV for writing
        with open(OUTPUT_LINES_FILE, 'w', encoding='utf-8', newline='') as f_lines_csv:
            lines_writer = csv.writer(f_lines_csv)
            # Write header for the detailed lines file
            lines_writer.writerow(["file", "page", "line_text",
                                   "lang_code", "lang_corrected",
                                   "lang_score", "lang_score_corrected",
                                   "perplexity", "perplexity_corrected",
                                   "category", "corrected_text"])
            rows_processed = 0
            all_stats_chunks = []  # To store modified DataFrames for final stats file

            # 3. Process the input stats file in chunks
            print(f"Reading '{INPUT_FILE}' in chunks of {CHUNK_SIZE}...")
            reader = pd.read_csv(INPUT_FILE, chunksize=CHUNK_SIZE)

            for chunk_df in reader:
                if "path" not in chunk_df.columns or "file" not in chunk_df.columns or "page" not in chunk_df.columns:
                    raise ValueError("Input CSV must have 'file', 'page', and 'path' columns.")

                new_stats_rows = []  # To build the new stats chunk

                # 4. Iterate row-by-row (per page) within the chunk
                for idx, row in chunk_df.iterrows():
                    file_id = str(row['file'])
                    page_id = str(row['page'])
                    xml_path = row['path']

                    # --- A. Extract Text from ALTO ---
                    page_lines = get_text_from_alto(xml_path)
                    total_text = " ".join(page_lines)

                    # --- B. Save raw text file ---
                    page_text_file = Path(OUTPUT_TEXT_DIR) / f"{file_id}-{page_id}.txt"
                    try:
                        with open(page_text_file, 'w', encoding='utf-8') as f_txt:
                            f_txt.write("\n".join(page_lines))
                    except Exception as e:
                        print(f"\n[Warning] Failed to write text file {page_text_file}: {e}", file=sys.stderr)

                    # --- C. Pre-calculate Page-Level stats ---
                    # These are used as context for 'Short' lines
                    try:
                        total_text_perplexity = calculate_perplexity(total_text, model_causal, tokenizer_causal)
                        labels, scores = model_fasttext.predict(total_text, k=1)
                        total_lang_code = labels[0].replace("__label__", "")
                        total_score1 = float(scores[0])
                        total_corrected_text, _ = autocorrect_text(total_text, total_lang_code)

                    except Exception as e:
                        total_corrected_text = ""
                        total_lang_code = ""
                        total_score1 = 0.0
                        total_text_perplexity = 0.0
                        print(
                            f"\n[Error] Model prediction failed for page text in file '{file_id}', page '{page_id}': {e}",
                            file=sys.stderr)

                    # --- D. Process each line on the page ---
                    line_counts = {"Clear": 0, "Noisy": 0, "Trash": 0, "Non-text": 0, "Empty": 0, "Rough": 0,
                                   "Short": 0}

                    for line in page_lines:
                        # Classify the line, passing in page-level stats as context
                        result = classify_line(line, model_ft=model_fasttext, model_ppl=model_causal,
                                               tokenizer_ppl=tokenizer_causal,
                                               corrected_page_text=total_corrected_text,
                                               corrected_page_lang=total_lang_code,
                                               corrected_page_lang_score=total_score1,
                                               corrected_page_perplexity=total_text_perplexity)

                        # Add to counts
                        if result['category'] in line_counts:
                            line_counts[result['category']] += 1
                        else:
                            # This handles 'N/A' or other unexpected categories
                            line_counts[result['category']] = 1

                        # Write to detailed CSV
                        lines_writer.writerow([
                            file_id, page_id, result['text'], result['lang_code'], result['lang_corrected'],
                            result['lang_score'], result['lang_score_corrected'],
                            result['perplexity'], result["perplexity_corrected"], result['category'],
                            result['corrected_text']
                        ])

                    # --- E. Prepare new row for the stats file ---
                    new_row = row.to_dict()
                    new_row['clear_lines'] = line_counts['Clear']
                    new_row['noisy_lines'] = line_counts['Noisy']
                    new_row['trash_lines'] = line_counts['Trash']
                    new_row['nontxt_lines'] = line_counts['Non-text']
                    new_row['empty_lines'] = line_counts['Empty']
                    new_row['rough_lines'] = line_counts['Rough']
                    new_row['short_lines'] = line_counts['Short']
                    new_stats_rows.append(new_row)

                    # --- F. Log progress ---
                    rows_processed += 1
                    percentage = (rows_processed / total_rows) * 100
                    sys.stdout.write(
                        f"\rProgress: {rows_processed:,}/{total_rows:,} pages ({percentage:.1f}%) processed.")
                    sys.stdout.flush()

                # Add the modified chunk to our list
                all_stats_chunks.append(pd.DataFrame(new_stats_rows))

        # --- 5. Post-Processing and Saving ---
        print("\nProcessing complete.")

        # --- A. Save the main stats file ---
        if all_stats_chunks:
            print("Consolidating stats file...")
            final_stats_df = pd.concat(all_stats_chunks, ignore_index=True)

            # Sort the stats file by file and page
            print(f"Sorting '{OUTPUT_STATS_FILE}' by file, page...")
            final_stats_df.sort_values(by=["file", "page"], inplace=True)

            final_stats_df.to_csv(OUTPUT_STATS_FILE, index=False, header=True)
            print(f"Updated stats with line counts saved to: {OUTPUT_STATS_FILE}")
        else:
            print("No data processed, stats file not created.")

        # --- B. Sort the detailed lines file ---
        # The file is now closed, so we can re-read it with pandas, sort, and overwrite.
        # This is more memory-efficient than holding all lines in memory.
        print(f"Sorting '{OUTPUT_LINES_FILE}' by file, page, and category. This may take a moment...")
        try:
            df_lines = pd.read_csv(OUTPUT_LINES_FILE)
            df_lines.sort_values(by=["file", "page", "category"], inplace=True)
            df_lines.to_csv(OUTPUT_LINES_FILE, index=False, header=True)
            print("Detailed lines file sorting complete.")
            print(f"Detailed line-by-line analysis saved to: {OUTPUT_LINES_FILE}")
        except Exception as e:
            print(f"\n[Error] Failed to sort {OUTPUT_LINES_FILE}: {e}", file=sys.stderr)
            print("The unsorted file is still available.")

        print(f"Raw text files saved in: {OUTPUT_TEXT_DIR}/")

    except FileNotFoundError:
        print(f"\n[Error] Input file not found at '{INPUT_FILE}'", file=sys.stderr)
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()